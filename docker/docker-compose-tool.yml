services:
  cllm_tool:
    build: .
    image: "cllm:v0"
    container_name: "cllm_tool"
    restart: "unless-stopped"
    ports:
      - "10004:10004"
      - "10005:10005"
      - "10024:10024"
    volumes:
      - ../model_zoo:/root/ControlLLM/model_zoo
      - ../certificate:/root/ControlLLM/certificate
      - ../client_resources:/root/ControlLLM/client_resources
      - ~/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ~/nltk_data:/root/nltk_data
    environment:
      - CLLM_SERVICES_PORT:10004
      - TOG_SERVICE_PORT:10005
      - OPENAI_API_KEY
      - OPENAI_BASE_URL
      - WEATHER_API_KEY
      - HF_ENDPOINT
      - CLIENT_ROOT:./client_resources
      - SERVER_ROOT:./server_resources
      - NVIDIA_VISIBLE_DEVICES:1
    network_mode: "host"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3', '4']
              capabilities: [gpu]
    entrypoint: "python"
    command:
      - "-m"
      - "cllm.services.launch"
      - "--port"
      - "10004"
      - "--host"
      - "0.0.0.0"